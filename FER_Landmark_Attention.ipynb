{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1))\n",
    "\n",
    "        cv2.imshow('MediaPipe FaceMesh', image)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44391f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_threshold = 0.5  # adjust this as needed\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        continue\n",
    "\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # extract the mouth landmarks\n",
    "            mouth_landmarks = face_landmarks.landmark[mp_face_mesh.FACEMESH_CONTOURS[13:23]]\n",
    "\n",
    "            # compute the distance between the corners of the mouth\n",
    "            mouth_width = (mouth_landmarks[12].x - mouth_landmarks[4].x) * image.shape[1]\n",
    "\n",
    "            # detect a smile if the mouth is open wider than a certain threshold\n",
    "            if mouth_width > smile_threshold:\n",
    "                cv2.putText(image, \"Smiling\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1))\n",
    "\n",
    "    cv2.imshow('MediaPipe FaceMesh', image)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c67f5f6d",
   "metadata": {},
   "source": [
    "# Started from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6426cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[45763]: Class CaptureDelegate is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x13df765a0) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x13ad24860). One of the two will be used. Which one is undefined.\n",
      "objc[45763]: Class CVWindow is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x13df765f0) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x12ffbca68). One of the two will be used. Which one is undefined.\n",
      "objc[45763]: Class CVView is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x13df76618) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x12ffbca90). One of the two will be used. Which one is undefined.\n",
      "objc[45763]: Class CVSlider is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x13df76640) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x12ffbcab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71d1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'sad', 'fear', 'surprise', 'neutral', 'angry', 'disgust']\n"
     ]
    }
   ],
   "source": [
    "test_dir = 'data/test/'\n",
    "train_dir = 'data/train/'\n",
    "classes = os.listdir(train_dir)\n",
    "num_classes = len(classes)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abb52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "def detect_landmarks(image):\n",
    "    # Initialize the FaceMesh model with default parameters\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "        \n",
    "        # Convert the image to RGB format and feed it to the model\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        # Initialize an empty tensor to store the landmarks\n",
    "        landmarks = torch.zeros(1, 468, 2, dtype=torch.float32)\n",
    "        \n",
    "        # If a face is detected, extract its landmarks and save them in the tensor\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for i, landmark in enumerate(face_landmarks.landmark):\n",
    "                    landmarks[0][i][0] = landmark.x\n",
    "                    landmarks[0][i][1] = landmark.y\n",
    "        \n",
    "        # Convert the RGB image back to BGR format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Return the image and landmarks as separate PyTorch tensors\n",
    "        return torch.tensor(image, dtype=torch.float32), landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687d3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 48, 3])\n",
      "torch.Size([1, 468, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "image = cv2.imread('data/test/angry/angry_1.jpg')\n",
    "image, landmarks = detect_landmarks(image)\n",
    "\n",
    "print(image.shape) #torch.Size([48, 48, 3])\n",
    "print(landmarks.shape) #torch.Size([1, 468, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc48fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(self.path)\n",
    "        self.files = []\n",
    "\n",
    "        for c in self.classes:\n",
    "            class_folder = os.path.join(self.path, c)\n",
    "            if os.path.isdir(class_folder):  # Add this line to check if the item is a directory\n",
    "                for file in os.listdir(class_folder):\n",
    "                    self.files.append((os.path.join(class_folder, file), self.classes.index(c)))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img, landmarks = detect_landmarks(img)\n",
    "        img = Image.fromarray(img.numpy().astype(np.uint8))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, landmarks, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e178645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_model, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_model // num_heads\n",
    "\n",
    "#         self.W_q = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.W_k = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.W_v = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, x, landmarks):\n",
    "#         N = x.shape[0]\n",
    "        \n",
    "#         # Concatenate the features with the landmarks\n",
    "#         x = torch.cat((x, landmarks), dim=1)\n",
    "\n",
    "#         Q = self.W_q(x).view(N, self.num_heads, self.head_dim)\n",
    "#         K = self.W_k(x).view(N, self.num_heads, self.head_dim)\n",
    "#         V = self.W_v(x).view(N, self.num_heads, self.head_dim)\n",
    "\n",
    "#         energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
    "#         attention = torch.softmax(energy / (self.d_model ** 0.5), dim=3)\n",
    "#         out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, V]).contiguous()\n",
    "#         out = out.view(N, self.d_model)\n",
    "#         out = self.fc_out(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d283a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, landmarks):\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # Concatenate the features with the landmarks\n",
    "        x = torch.cat((x, landmarks), dim=1)\n",
    "\n",
    "        Q = self.W_q(x).view(N, self.num_heads, self.head_dim)\n",
    "        K = self.W_k(x).view(N, self.num_heads, self.head_dim)\n",
    "        V = self.W_v(x).view(N, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Compute attention scores using matrix multiplication\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "        # Scale the scores\n",
    "        scaled_scores = scores / (self.d_model ** 0.5)\n",
    "\n",
    "        # Apply softmax to get the attention weights\n",
    "        attention_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "        # Compute the output using matrix multiplication\n",
    "        out = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Concatenate the heads and apply the output linear layer\n",
    "        out = out.transpose(0, 1).reshape(N, -1)\n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32162ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkAttentionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # Update input dimensions based on the 64x64 image size\n",
    "        self.attention = MultiHeadAttention(128, 8)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x, landmarks):\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 16 * 16)  # Update the view dimensions based on the 64x64 image size\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.attention(x, landmarks.view(-1, 468 * 2))  # Pass landmarks as input to the MultiHeadAttention module\n",
    "        x = nn.functional.relu(x)\n",
    "        class_output = self.fc3(x)\n",
    "        return class_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5bf0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "\n",
    "train_dataset = LandmarkDataset(train_dir, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = LandmarkDataset(test_dir, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LandmarkAttentionCNN(num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a07a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, landmarks, targets) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print(data.shape)\n",
    "        # forward\n",
    "        outputs = model(data, landmarks)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "        # update metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31561c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, landmarks, targets) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            #print(data.shape)\n",
    "            # forward\n",
    "            outputs = model(data, landmarks)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # update metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d74a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_dataloader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db09c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
