{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "def detect_landmarks(image):\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "        \n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image_with_landmarks = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image_with_landmarks,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1))\n",
    "                    \n",
    "        return image_with_landmarks\n",
    "\n",
    "# Load an input image using OpenCV\n",
    "image = cv2.imread('/Users/Shared/D/Applied AI Solutions/DL2/FER/data/train/happy/happy_24.jpg')\n",
    "# increase the size of the image\n",
    "image = cv2.resize(image, (256, 256))\n",
    "\n",
    "# Call the detect_landmarks function to obtain the image with landmark overlay\n",
    "image_with_landmarks = detect_landmarks(image)\n",
    "\n",
    "# Display the image with landmark overlay\n",
    "cv2.imshow('MediaPipe FaceMesh', image_with_landmarks)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44391f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_threshold = 0.5  # adjust this as needed\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        continue\n",
    "\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = mp_face_mesh.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # extract the mouth landmarks\n",
    "            mouth_landmarks = face_landmarks.landmark[mp_face_mesh.FACEMESH_CONTOURS[13:23]]\n",
    "\n",
    "            # compute the distance between the corners of the mouth\n",
    "            mouth_width = (mouth_landmarks[12].x - mouth_landmarks[4].x) * image.shape[1]\n",
    "\n",
    "            # detect a smile if the mouth is open wider than a certain threshold\n",
    "            if mouth_width > smile_threshold:\n",
    "                cv2.putText(image, \"Smiling\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1))\n",
    "\n",
    "    cv2.imshow('MediaPipe FaceMesh', image)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c67f5f6d",
   "metadata": {},
   "source": [
    "# Started from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6426cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[62928]: Class CaptureDelegate is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x1637365a0) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x1602d4860). One of the two will be used. Which one is undefined.\n",
      "objc[62928]: Class CVWindow is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x1637365f0) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x137d3ca68). One of the two will be used. Which one is undefined.\n",
      "objc[62928]: Class CVView is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x163736618) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x137d3ca90). One of the two will be used. Which one is undefined.\n",
      "objc[62928]: Class CVSlider is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x163736640) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x137d3cab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71d1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'sad', 'fear', 'surprise', 'neutral', 'angry', 'disgust']\n"
     ]
    }
   ],
   "source": [
    "test_dir = 'data/test/'\n",
    "train_dir = 'data/train/'\n",
    "classes = os.listdir(train_dir)\n",
    "num_classes = len(classes)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abb52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "def detect_landmarks(image):\n",
    "    # Initialize the FaceMesh model with default parameters\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "        \n",
    "        # Convert the image to RGB format and feed it to the model\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        # Initialize an empty tensor to store the landmarks\n",
    "        landmarks = torch.zeros(1, 468, 2, dtype=torch.float32)\n",
    "        \n",
    "        # If a face is detected, extract its landmarks and save them in the tensor\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for i, landmark in enumerate(face_landmarks.landmark):\n",
    "                    landmarks[0][i][0] = landmark.x\n",
    "                    landmarks[0][i][1] = landmark.y\n",
    "        \n",
    "        # Convert the RGB image back to BGR format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Return the image and landmarks as separate PyTorch tensors\n",
    "        return torch.tensor(image, dtype=torch.float32), landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687d3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 48, 3])\n",
      "torch.Size([1, 468, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "image = cv2.imread('data/test/angry/angry_1.jpg')\n",
    "image, landmarks = detect_landmarks(image)\n",
    "\n",
    "print(image.shape) #torch.Size([48, 48, 3])\n",
    "print(landmarks.shape) #torch.Size([1, 468, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc48fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(self.path)\n",
    "        self.files = []\n",
    "\n",
    "        for c in self.classes:\n",
    "            class_folder = os.path.join(self.path, c)\n",
    "            if os.path.isdir(class_folder):  # Add this line to check if the item is a directory\n",
    "                for file in os.listdir(class_folder):\n",
    "                    self.files.append((os.path.join(class_folder, file), self.classes.index(c)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img, landmarks = detect_landmarks(img)\n",
    "        img = Image.fromarray(img.numpy().astype(np.uint8))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, landmarks, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e178645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_model, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_model // num_heads\n",
    "\n",
    "#         self.W_q = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.W_k = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.W_v = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, x, landmarks):\n",
    "#         N = x.shape[0]\n",
    "        \n",
    "#         # Concatenate the features with the landmarks\n",
    "#         x = torch.cat((x, landmarks), dim=1)\n",
    "\n",
    "#         Q = self.W_q(x).view(N, self.num_heads, self.head_dim)\n",
    "#         K = self.W_k(x).view(N, self.num_heads, self.head_dim)\n",
    "#         V = self.W_v(x).view(N, self.num_heads, self.head_dim)\n",
    "\n",
    "#         energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
    "#         attention = torch.softmax(energy / (self.d_model ** 0.5), dim=3)\n",
    "#         out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, V]).contiguous()\n",
    "#         out = out.view(N, self.d_model)\n",
    "#         out = self.fc_out(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d283a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, landmarks):\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # Concatenate the features with the landmarks\n",
    "        x = torch.cat((x, landmarks), dim=1)\n",
    "\n",
    "        Q = self.W_q(x).view(N, self.num_heads, self.head_dim)\n",
    "        K = self.W_k(x).view(N, self.num_heads, self.head_dim)\n",
    "        V = self.W_v(x).view(N, self.num_heads, self.head_dim)\n",
    "\n",
    "        \"\"\"\n",
    "            This part has no idea\n",
    "        \"\"\"\n",
    "        # Compute attention scores using matrix multiplication\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        # Scale the scores\n",
    "        scaled_scores = scores / (self.d_model ** 0.5)\n",
    "        # Apply softmax to get the attention weights\n",
    "        attention_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "        # Compute the output using matrix multiplication\n",
    "        out = torch.matmul(attention_weights, V)\n",
    "        # Concatenate the heads and apply the output linear layer\n",
    "        out = out.transpose(0, 1).reshape(N, -1)\n",
    "\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6efea8f",
   "metadata": {},
   "source": [
    "# Notes\n",
    "1. Paralell branching on the CNN model can be tested!\n",
    "2. Balance of the dataset needs to be checked!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32162ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkAttentionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Block-1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block-2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block-3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block-4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block-5\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention = MultiHeadAttention(128, 8)\n",
    "\n",
    "        # Block-6\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Block-7\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, landmarks):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        x = self.fc1(x)\n",
    "        x = self.attention(x, landmarks.view(-1, 468 * 2))  # Pass landmarks as input to the MultiHeadAttention module\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        class_output = self.fc3(x)\n",
    "        return class_output\n",
    "\n",
    "\n",
    "num_classes = 7  # Update this value based on your specific problem\n",
    "model = LandmarkAttentionCNN(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ca7638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input size: 0.40 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_size = [(3, 64, 64), (468, 2)]\n",
    "batch_size = 8\n",
    "\n",
    "total_input_size = sum([np.prod(size) for size in input_size]) * batch_size * 4. / (1024 ** 2.)\n",
    "print(f\"Total input size: {total_input_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab8a7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LandmarkAttentionCNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (attention): MultiHeadAttention(\n",
       "    (W_q): Linear(in_features=1064, out_features=128, bias=False)\n",
       "    (W_k): Linear(in_features=1064, out_features=128, bias=False)\n",
       "    (W_v): Linear(in_features=1064, out_features=128, bias=False)\n",
       "    (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc3): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the model before training\n",
    "model = LandmarkAttentionCNN(7)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5bf0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "\n",
    "train_dataset = LandmarkDataset(train_dir, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = LandmarkDataset(test_dir, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LandmarkAttentionCNN(num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a07a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, landmarks, targets) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print(data.shape)\n",
    "        # forward\n",
    "        outputs = model(data, landmarks)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "        # update metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31561c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, landmarks, targets) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            #print(data.shape)\n",
    "            # forward\n",
    "            outputs = model(data, landmarks)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # update metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "828ea399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d74a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_dataloader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db09c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
