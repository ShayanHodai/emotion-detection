{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c67f5f6d",
   "metadata": {},
   "source": [
    "# Started from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6426cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[85365]: Class CaptureDelegate is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c7765a0) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x13f4d8860). One of the two will be used. Which one is undefined.\n",
      "objc[85365]: Class CVWindow is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c7765f0) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x12dd54a68). One of the two will be used. Which one is undefined.\n",
      "objc[85365]: Class CVView is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c776618) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x12dd54a90). One of the two will be used. Which one is undefined.\n",
      "objc[85365]: Class CVSlider is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c776640) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x12dd54ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "import heapq\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71d1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'data/test/'\n",
    "train_dir = 'data/train/'\n",
    "classes = os.listdir(train_dir)\n",
    "test_cls = os.listdir(train_dir)\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "184cd348",
   "metadata": {},
   "source": [
    "# Landmark detection from images and correlation of those landmark positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_distances(landmarks):\n",
    "    num_landmarks = landmarks.shape[0]\n",
    "    pairwise_distances = np.zeros((num_landmarks, num_landmarks))\n",
    "    for i, j in itertools.product(range(num_landmarks), repeat=2):\n",
    "        if i != j:\n",
    "            pairwise_distances[i, j] = np.linalg.norm(landmarks[i] - landmarks[j])\n",
    "\n",
    "    pairwise_distances = pairwise_distances[np.triu_indices(num_landmarks, k=1)]\n",
    "    return pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abb52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "def detect_landmarks(image):\n",
    "    # Initialize the FaceMesh model with default parameters\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "        \n",
    "        # Convert the image to RGB format and feed it to the model\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        # Initialize an empty tensor to store the landmarks\n",
    "        landmarks = torch.zeros(1, 109278, dtype=torch.float32)  # Use the correct landmarks shape\n",
    "        \n",
    "        # If a face is detected, extract its landmarks and save them in the tensor\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                landmark_coords = np.zeros((468, 2))\n",
    "                for i, landmark in enumerate(face_landmarks.landmark):\n",
    "                    landmark_coords[i, 0] = landmark.x\n",
    "                    landmark_coords[i, 1] = landmark.y\n",
    "                \n",
    "                pairwise_distances = compute_pairwise_distances(landmark_coords)\n",
    "                landmarks[0] = torch.tensor(pairwise_distances, dtype=torch.float32)\n",
    "        \n",
    "        # Convert the RGB image back to BGR format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Return the image and landmarks as separate PyTorch tensors\n",
    "        return torch.tensor(image, dtype=torch.float32), landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687d3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 48, 3])\n",
      "torch.Size([1, 109278])\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('data/test/angry/angry_1.jpg')\n",
    "image, landmarks = detect_landmarks(image)\n",
    "\n",
    "print(image.shape)\n",
    "print(landmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc48fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        for label in os.listdir(root_dir):\n",
    "            class_dir = os.path.join(root_dir, label)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                image = cv2.imread(img_path)\n",
    "                _, landmarks = detect_landmarks(image)\n",
    "                self.data.append((img_path, landmarks))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, landmarks = self.data[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, landmarks, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_landmarks = 468\n",
    "landmarks_length = (num_landmarks * (num_landmarks - 1)) // 2\n",
    "\n",
    "print(f'Number of landmarks: {num_landmarks}')\n",
    "print(f'Length of the landmark vector: {landmarks_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = LandmarkDataset(train_dir, transform=transform)\n",
    "val_dataset = LandmarkDataset(test_dir, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca to the dataset\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(train_dataset.data[0][1].numpy())\n",
    "print(pca.n_components_, \"\\n ------------\")\n",
    "print(pca.explained_variance_ratio_, \"\\n ------------\")\n",
    "print(pca.singular_values_, \"\\n ------------\")\n",
    "print(pca.components_, \"\\n ------------\")\n",
    "print(pca.mean_, \"\\n ------------\")\n",
    "print(pca.noise_variance_, \"\\n ------------\")\n",
    "print(pca.get_covariance(), \"\\n ------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d283a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model + landmarks_length, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model + landmarks_length, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model + landmarks_length, d_model, bias=False)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, landmarks):\n",
    "        N = x.shape[0]\n",
    "\n",
    "        # Concatenate the features with the landmarks\n",
    "        x = torch.cat((x, landmarks), dim=1)\n",
    "\n",
    "        Q = self.W_q(x).view(N, self.num_heads, self.head_dim)\n",
    "        K = self.W_k(x).view(N, self.num_heads, self.head_dim)\n",
    "        V = self.W_v(x).view(N, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Compute attention scores using matrix multiplication\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        # Scale the scores\n",
    "        scaled_scores = scores / (self.d_model ** 0.5)\n",
    "        # Apply softmax to get the attention weights\n",
    "        attention_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "        # Compute the output using matrix multiplication\n",
    "        out = torch.matmul(attention_weights, V)\n",
    "        # Concatenate the heads and apply the output linear layer\n",
    "        out = out.transpose(0, 1).reshape(N, -1)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6efea8f",
   "metadata": {},
   "source": [
    "# Notes\n",
    "1. Paralell branching on the CNN model can be tested!\n",
    "2. Balance of the dataset needs to be checked!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32162ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkAttentionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Block-1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block-2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block-3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block-4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block-5\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention = MultiHeadAttention(128, 8)\n",
    "\n",
    "        # Block-6\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Block-7\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, landmarks):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Pass landmarks as input to the MultiHeadAttention module\n",
    "        x = self.attention(x, landmarks.view(-1, landmarks_length))\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        class_output = self.fc3(x)\n",
    "        return class_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca52362",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7  # Update this value based on your specific problem\n",
    "model = LandmarkAttentionCNN(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [(1, 3, 48, 48), (1, landmarks_length)]\n",
    "batch_size = 4\n",
    "\n",
    "total_input_size = sum([np.prod(size) for size in input_size]) * batch_size * 4. / (1024 ** 2.)\n",
    "print(f\"Total input size: {total_input_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model2txt\n",
    "model = LandmarkAttentionCNN(7)\n",
    "with open('model.txt', 'w') as f:\n",
    "    f.write(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488203d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_landmarks = 468\n",
    "landmarks_length = (num_landmarks * (num_landmarks - 1)) // 2\n",
    "\n",
    "model = LandmarkAttentionCNN(num_classes, landmarks_length).to(device)  # Pass landmarks_length as an argument\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, landmarks, targets) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print(data.shape)\n",
    "        # forward\n",
    "        outputs = model(data, landmarks)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "        # update metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31561c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, landmarks, targets) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            #print(data.shape)\n",
    "            # forward\n",
    "            outputs = model(data, landmarks)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # update metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ea399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d74a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_dataloader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db09c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
