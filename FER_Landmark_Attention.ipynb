{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1))\n",
    "\n",
    "        cv2.imshow('MediaPipe FaceMesh', image)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44391f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_threshold = 0.5  # adjust this as needed\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        continue\n",
    "\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # extract the mouth landmarks\n",
    "            mouth_landmarks = face_landmarks.landmark[mp_face_mesh.FACEMESH_CONTOURS[13:23]]\n",
    "\n",
    "            # compute the distance between the corners of the mouth\n",
    "            mouth_width = (mouth_landmarks[12].x - mouth_landmarks[4].x) * image.shape[1]\n",
    "\n",
    "            # detect a smile if the mouth is open wider than a certain threshold\n",
    "            if mouth_width > smile_threshold:\n",
    "                cv2.putText(image, \"Smiling\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1))\n",
    "\n",
    "    cv2.imshow('MediaPipe FaceMesh', image)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c67f5f6d",
   "metadata": {},
   "source": [
    "# Started from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6426cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[46864]: Class CaptureDelegate is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x159fb25a0) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x13d7d0860). One of the two will be used. Which one is undefined.\n",
      "objc[46864]: Class CVWindow is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x159fb25f0) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11e5d8a68). One of the two will be used. Which one is undefined.\n",
      "objc[46864]: Class CVView is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x159fb2618) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11e5d8a90). One of the two will be used. Which one is undefined.\n",
      "objc[46864]: Class CVSlider is implemented in both /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x159fb2640) and /Users/sertanavdan/miniconda3/envs/torch/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11e5d8ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71d1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'sad', 'fear', 'surprise', 'neutral', 'angry', 'disgust']\n"
     ]
    }
   ],
   "source": [
    "test_dir = 'data/test/'\n",
    "train_dir = 'data/train/'\n",
    "classes = os.listdir(train_dir)\n",
    "num_classes = len(classes)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(self.path)\n",
    "        self.files = []\n",
    "\n",
    "        for c in self.classes:\n",
    "            class_folder = os.path.join(self.path, c)\n",
    "            if os.path.isdir(class_folder):  # Add this line to check if the item is a directory\n",
    "                for file in os.listdir(class_folder):\n",
    "                    self.files.append((os.path.join(class_folder, file), self.classes.index(c)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img, landmarks = detect_landmarks(img)\n",
    "        img = Image.fromarray(img.numpy().astype(np.uint8))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, landmarks, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abb52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "def detect_landmarks(image):\n",
    "    # Initialize the FaceMesh model with default parameters\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "        \n",
    "        # Convert the image to RGB format and feed it to the model\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        # Initialize an empty tensor to store the landmarks\n",
    "        landmarks = torch.zeros(1, 468, 2, dtype=torch.float32)\n",
    "        \n",
    "        # If a face is detected, extract its landmarks and save them in the tensor\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for i, landmark in enumerate(face_landmarks.landmark):\n",
    "                    landmarks[0][i][0] = landmark.x\n",
    "                    landmarks[0][i][1] = landmark.y\n",
    "        \n",
    "        # Convert the RGB image back to BGR format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Return the image and landmarks as separate PyTorch tensors\n",
    "        return torch.tensor(image, dtype=torch.float32), landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687d3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 48, 3])\n",
      "torch.Size([1, 468, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "image = cv2.imread('data/test/angry/angry_1.jpg')\n",
    "image, landmarks = detect_landmarks(image)\n",
    "\n",
    "print(image.shape) #torch.Size([48, 48, 3])\n",
    "print(landmarks.shape) #torch.Size([1, 468, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e178645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_model, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_model // num_heads\n",
    "\n",
    "#         self.W_q = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.W_k = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.W_v = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "#         self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, x, landmarks):\n",
    "#         N = x.shape[0]\n",
    "        \n",
    "#         # Concatenate the features with the landmarks\n",
    "#         x = torch.cat((x, landmarks), dim=1)\n",
    "\n",
    "#         Q = self.W_q(x).view(N, self.num_heads, self.head_dim)\n",
    "#         K = self.W_k(x).view(N, self.num_heads, self.head_dim)\n",
    "#         V = self.W_v(x).view(N, self.num_heads, self.head_dim)\n",
    "\n",
    "#         energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
    "#         attention = torch.softmax(energy / (self.d_model ** 0.5), dim=3)\n",
    "#         out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, V]).contiguous()\n",
    "#         out = out.view(N, self.d_model)\n",
    "#         out = self.fc_out(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d283a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model + 468 * 2, d_model, bias=False)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, landmarks):\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # Concatenate the features with the landmarks\n",
    "        x = torch.cat((x, landmarks), dim=1)\n",
    "\n",
    "        Q = self.W_q(x).view(N, self.num_heads, self.head_dim)\n",
    "        K = self.W_k(x).view(N, self.num_heads, self.head_dim)\n",
    "        V = self.W_v(x).view(N, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Compute attention scores using matrix multiplication\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "        # Scale the scores\n",
    "        scaled_scores = scores / (self.d_model ** 0.5)\n",
    "\n",
    "        # Apply softmax to get the attention weights\n",
    "        attention_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "        # Compute the output using matrix multiplication\n",
    "        out = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Concatenate the heads and apply the output linear layer\n",
    "        out = out.transpose(0, 1).reshape(N, -1)\n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32162ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkAttentionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # Update input dimensions based on the 64x64 image size\n",
    "        self.attention = MultiHeadAttention(128, 8)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x, landmarks):\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 16 * 16)  # Update the view dimensions based on the 64x64 image size\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.attention(x, landmarks.view(-1, 468 * 2))  # Pass landmarks as input to the MultiHeadAttention module\n",
    "        x = nn.functional.relu(x)\n",
    "        class_output = self.fc3(x)\n",
    "        return class_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5bf0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "\n",
    "train_dataset = LandmarkDataset(train_dir, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = LandmarkDataset(test_dir, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LandmarkAttentionCNN(num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a07a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, landmarks, targets) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # print(data.shape)\n",
    "        # forward\n",
    "        outputs = model(data, landmarks)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "        # update metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31561c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, landmarks, targets) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            #print(data.shape)\n",
    "            # forward\n",
    "            outputs = model(data, landmarks)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # update metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "828ea399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76d74a08",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 7 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Shared/D/Applied AI Solutions/DL2/FER/FER_Landmark_Attention.ipynb Cell 16\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_epoch(model, train_dataloader, criterion, optimizer, device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m validate_epoch(model, val_dataloader, criterion, device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(val_loss)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val Acc: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/Shared/D/Applied AI Solutions/DL2/FER/FER_Landmark_Attention.ipynb Cell 16\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#print(data.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(data, landmarks)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# update metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Shared/D/Applied%20AI%20Solutions/DL2/FER/FER_Landmark_Attention.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 7 is out of bounds."
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_dataloader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db09c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
