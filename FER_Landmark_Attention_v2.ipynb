{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67f5f6d",
   "metadata": {},
   "source": [
    "# Started from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6426cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /usr/lib/python3/dist-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "import heapq\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71d1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'data/merged/test/'\n",
    "train_dir = 'data/merged/train/'\n",
    "\n",
    "classes = os.listdir(train_dir)\n",
    "test_cls = os.listdir(train_dir)\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dc0934-8a23-4c76-a883-9b27a75b2815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7 \n",
      "Classes: ['fear', 'angry', 'sad', 'neutral', 'surprise', 'disgust', 'happy']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Number of classes: {num_classes}',\n",
    "    f'\\nClasses: {classes}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983c4b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Train--------\n",
      "fear: 3288\n",
      "angry: 3196\n",
      "sad: 3852\n",
      "neutral: 3948\n",
      "surprise: 3456\n",
      "disgust: 5674\n",
      "happy: 5786\n",
      "\n",
      "--------Test--------\n",
      "fear: 810\n",
      "angry: 800\n",
      "sad: 979\n",
      "neutral: 1018\n",
      "surprise: 873\n",
      "disgust: 1390\n",
      "happy: 1430\n"
     ]
    }
   ],
   "source": [
    "print(\"--------Train--------\")\n",
    "# check the number of images in each class\n",
    "for cls in classes:\n",
    "    print(f'{cls}: {len(os.listdir(train_dir + cls))}')\n",
    "\n",
    "print(\"\\n--------Test--------\")\n",
    "# check the number of images in each class\n",
    "for cls in classes:\n",
    "    print(f'{cls}: {len(os.listdir(test_dir + cls))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d54f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Train--------\n",
      "\n",
      "--------Test--------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imghdr\n",
    "import shutil\n",
    "\n",
    "def delete_irrelevant_files(directory, classes):\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(directory, cls)\n",
    "        with os.scandir(class_dir) as entries:\n",
    "            for entry in entries:\n",
    "                # Delete directories\n",
    "                if entry.is_dir():\n",
    "                    print(f'Deleting directory: {entry.path}')\n",
    "                    shutil.rmtree(entry.path)\n",
    "\n",
    "                # Check if the file is empty and delete it\n",
    "                elif os.path.getsize(entry.path) == 0:\n",
    "                    print(f'Deleting empty file: {entry.path}')\n",
    "                    os.remove(entry.path)\n",
    "\n",
    "                # Check if the file is not an image and delete it\n",
    "                elif not imghdr.what(entry.path):\n",
    "                    print(f'Deleting non-image file: {entry.path}')\n",
    "                    os.remove(entry.path)\n",
    "\n",
    "print(\"--------Train--------\")\n",
    "delete_irrelevant_files(train_dir, classes)\n",
    "\n",
    "print(\"\\n--------Test--------\")\n",
    "delete_irrelevant_files(test_dir, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cd348",
   "metadata": {},
   "source": [
    "# Landmark detection from images and correlation of those landmark positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687d3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image input size: 0.84 MB\n"
     ]
    }
   ],
   "source": [
    "image_size = torch.Size([48, 48, 3])\n",
    "batch_size = 32\n",
    "\n",
    "# Compute the total size of the image input\n",
    "image_input_size = np.prod(image_size) * batch_size * 4. / (1024 ** 2.)\n",
    "print(f\"Image input size: {image_input_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc48fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []  # new list to store the label index\n",
    "        for i, label in enumerate(os.listdir(root_dir)):\n",
    "            class_dir = os.path.join(root_dir, label)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                image = cv2.imread(img_path)\n",
    "                self.data.append(image)\n",
    "                self.labels.append(i)  # add the label index for this image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]  # get the label index for this image\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label  # return the image and label index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde4c05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(48, scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = LandmarkDataset(train_dir, transform=transform)\n",
    "val_dataset = LandmarkDataset(test_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e67fa6f-c829-4088-87a8-2a80c660a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=24)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b2c0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([256, 3, 48, 48]) torch.Size([256])\n",
      "0 torch.Size([256, 3, 48, 48]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# show shape of the train_dataloader\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    print(i, data[0].shape, data[1].shape)\n",
    "    break\n",
    "\n",
    "# show shape of the val_dataloader\n",
    "for i, data in enumerate(val_dataloader):\n",
    "    print(i, data[0].shape, data[1].shape)\n",
    "    break\n",
    "\n",
    "# torch.Size([4, 3, 48, 48]) torch.Size([4])\n",
    "# torch.Size([4, 3, 48, 48]) torch.Size([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d283a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        qk = torch.matmul(query, key.transpose(-2, -1))\n",
    "        dk = query.size(-1)\n",
    "        scaled_attention_logits = qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(query, key, value)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.input_dim)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32162ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialExpressionDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_heads):\n",
    "        super(FacialExpressionDetectionModel, self).__init__()\n",
    "        \n",
    "        # Block-1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Block-2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Block-3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # Block-4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.125)\n",
    "        )\n",
    "\n",
    "        # Block-5\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64 * 6 * 6, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention = MultiHeadAttention(128, num_heads)\n",
    "        # Change the output size to match the input size of fc2\n",
    "        self.attention_transform = nn.Linear(128, 64)\n",
    "\n",
    "        # Block-6\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 128),  # Update the input size to 64\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.125),\n",
    "        )\n",
    "\n",
    "        # Block-7\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output of the attention layer\n",
    "        x = self.attention_transform(x)\n",
    "        #print(x.shape)  # Add this line to print the shape\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ba4b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aca52362",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "num_heads = 32\n",
    "model = FacialExpressionDetectionModel(num_classes, num_heads).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab8a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_v2.txt', 'w') as f:\n",
    "    f.write(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a07a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data, targets in dataloader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Use autocast for mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Scale the loss and perform backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        running_corrects += torch.sum(preds == targets.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31561c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Use autocast for mixed precision\n",
    "            with autocast():\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            running_corrects += torch.sum(preds == targets.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "488203d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model = FacialExpressionDetectionModel(num_classes, num_heads).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "828ea399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c79dfa-79fd-493e-8382-5ab4c8c7de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d74a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 1.7894, Train Acc: 0.3694, Val Loss: 1.7838, Val Acc: 0.3729\n",
      "Epoch: 2/100, Train Loss: 1.7614, Train Acc: 0.3953, Val Loss: 1.7608, Val Acc: 0.3892\n",
      "Epoch: 3/100, Train Loss: 1.7375, Train Acc: 0.4186, Val Loss: 1.7276, Val Acc: 0.4264\n",
      "Epoch: 4/100, Train Loss: 1.7131, Train Acc: 0.4428, Val Loss: 1.7276, Val Acc: 0.4296\n",
      "Epoch: 5/100, Train Loss: 1.6962, Train Acc: 0.4590, Val Loss: 1.6859, Val Acc: 0.4688\n",
      "Epoch: 6/100, Train Loss: 1.6821, Train Acc: 0.4725, Val Loss: 1.6787, Val Acc: 0.4748\n",
      "Epoch: 7/100, Train Loss: 1.6698, Train Acc: 0.4842, Val Loss: 1.6621, Val Acc: 0.4940\n",
      "Epoch: 8/100, Train Loss: 1.6597, Train Acc: 0.4954, Val Loss: 1.6573, Val Acc: 0.4949\n",
      "Epoch: 9/100, Train Loss: 1.6628, Train Acc: 0.4943, Val Loss: 1.6564, Val Acc: 0.4997\n",
      "Epoch: 10/100, Train Loss: 1.6512, Train Acc: 0.5045, Val Loss: 1.6536, Val Acc: 0.5044\n",
      "Epoch: 11/100, Train Loss: 1.6443, Train Acc: 0.5125, Val Loss: 1.6380, Val Acc: 0.5190\n",
      "Epoch: 12/100, Train Loss: 1.6428, Train Acc: 0.5151, Val Loss: 1.6335, Val Acc: 0.5218\n",
      "Epoch: 13/100, Train Loss: 1.6393, Train Acc: 0.5179, Val Loss: 1.6357, Val Acc: 0.5223\n",
      "Epoch: 14/100, Train Loss: 1.6292, Train Acc: 0.5298, Val Loss: 1.6397, Val Acc: 0.5182\n",
      "Epoch: 15/100, Train Loss: 1.6370, Train Acc: 0.5201, Val Loss: 1.6284, Val Acc: 0.5275\n",
      "Epoch: 16/100, Train Loss: 1.6257, Train Acc: 0.5329, Val Loss: 1.6170, Val Acc: 0.5412\n",
      "Epoch: 17/100, Train Loss: 1.6201, Train Acc: 0.5395, Val Loss: 1.6168, Val Acc: 0.5432\n",
      "Epoch: 18/100, Train Loss: 1.6165, Train Acc: 0.5414, Val Loss: 1.6224, Val Acc: 0.5359\n",
      "Epoch: 19/100, Train Loss: 1.6172, Train Acc: 0.5409, Val Loss: 1.6190, Val Acc: 0.5388\n",
      "Epoch: 20/100, Train Loss: 1.6127, Train Acc: 0.5458, Val Loss: 1.6335, Val Acc: 0.5241\n",
      "Epoch: 21/100, Train Loss: 1.6238, Train Acc: 0.5347, Val Loss: 1.6071, Val Acc: 0.5493\n",
      "Epoch: 22/100, Train Loss: 1.6109, Train Acc: 0.5472, Val Loss: 1.6027, Val Acc: 0.5563\n",
      "Epoch: 23/100, Train Loss: 1.6060, Train Acc: 0.5527, Val Loss: 1.6109, Val Acc: 0.5460\n",
      "Epoch: 24/100, Train Loss: 1.6048, Train Acc: 0.5532, Val Loss: 1.6049, Val Acc: 0.5551\n",
      "Epoch: 25/100, Train Loss: 1.6050, Train Acc: 0.5534, Val Loss: 1.5972, Val Acc: 0.5599\n",
      "Epoch: 26/100, Train Loss: 1.6040, Train Acc: 0.5544, Val Loss: 1.6082, Val Acc: 0.5489\n",
      "Epoch: 27/100, Train Loss: 1.6006, Train Acc: 0.5592, Val Loss: 1.6229, Val Acc: 0.5351\n",
      "Epoch: 28/100, Train Loss: 1.5964, Train Acc: 0.5631, Val Loss: 1.5868, Val Acc: 0.5726\n",
      "Epoch: 29/100, Train Loss: 1.5948, Train Acc: 0.5635, Val Loss: 1.5811, Val Acc: 0.5801\n",
      "Epoch: 30/100, Train Loss: 1.5953, Train Acc: 0.5626, Val Loss: 1.6008, Val Acc: 0.5560\n",
      "Epoch: 31/100, Train Loss: 1.5934, Train Acc: 0.5671, Val Loss: 1.5820, Val Acc: 0.5759\n",
      "Epoch: 32/100, Train Loss: 1.5889, Train Acc: 0.5710, Val Loss: 1.5865, Val Acc: 0.5730\n",
      "Epoch: 33/100, Train Loss: 1.5926, Train Acc: 0.5660, Val Loss: 1.5832, Val Acc: 0.5773\n",
      "Epoch: 34/100, Train Loss: 1.5891, Train Acc: 0.5708, Val Loss: 1.5825, Val Acc: 0.5762\n",
      "Epoch: 35/100, Train Loss: 1.5868, Train Acc: 0.5730, Val Loss: 1.5861, Val Acc: 0.5738\n",
      "Epoch: 36/100, Train Loss: 1.5751, Train Acc: 0.5866, Val Loss: 1.5696, Val Acc: 0.5892\n",
      "Epoch: 37/100, Train Loss: 1.5707, Train Acc: 0.5888, Val Loss: 1.5627, Val Acc: 0.5956\n",
      "Epoch: 38/100, Train Loss: 1.5644, Train Acc: 0.5957, Val Loss: 1.5652, Val Acc: 0.5929\n",
      "Epoch: 39/100, Train Loss: 1.5680, Train Acc: 0.5915, Val Loss: 1.5660, Val Acc: 0.5930\n",
      "Epoch: 40/100, Train Loss: 1.5617, Train Acc: 0.5987, Val Loss: 1.5599, Val Acc: 0.5988\n",
      "Epoch: 41/100, Train Loss: 1.5557, Train Acc: 0.6046, Val Loss: 1.5596, Val Acc: 0.5997\n",
      "Epoch: 42/100, Train Loss: 1.5555, Train Acc: 0.6038, Val Loss: 1.5575, Val Acc: 0.6047\n",
      "Epoch: 43/100, Train Loss: 1.5547, Train Acc: 0.6067, Val Loss: 1.5619, Val Acc: 0.5963\n",
      "Epoch: 44/100, Train Loss: 1.5555, Train Acc: 0.6048, Val Loss: 1.5586, Val Acc: 0.6004\n",
      "Epoch: 45/100, Train Loss: 1.5549, Train Acc: 0.6064, Val Loss: 1.5571, Val Acc: 0.5996\n",
      "Epoch: 46/100, Train Loss: 1.5543, Train Acc: 0.6062, Val Loss: 1.5590, Val Acc: 0.6015\n",
      "Epoch: 47/100, Train Loss: 1.5553, Train Acc: 0.6049, Val Loss: 1.5564, Val Acc: 0.6011\n",
      "Epoch: 48/100, Train Loss: 1.5559, Train Acc: 0.6042, Val Loss: 1.5571, Val Acc: 0.6014\n",
      "Epoch: 49/100, Train Loss: 1.5552, Train Acc: 0.6053, Val Loss: 1.5595, Val Acc: 0.5985\n",
      "Epoch: 50/100, Train Loss: 1.5569, Train Acc: 0.6040, Val Loss: 1.5537, Val Acc: 0.6073\n",
      "Epoch: 51/100, Train Loss: 1.5556, Train Acc: 0.6047, Val Loss: 1.5575, Val Acc: 0.5992\n",
      "Epoch: 52/100, Train Loss: 1.5552, Train Acc: 0.6053, Val Loss: 1.5572, Val Acc: 0.6016\n",
      "Epoch: 53/100, Train Loss: 1.5547, Train Acc: 0.6063, Val Loss: 1.5594, Val Acc: 0.5986\n",
      "Epoch: 54/100, Train Loss: 1.5562, Train Acc: 0.6070, Val Loss: 1.5570, Val Acc: 0.6010\n",
      "Epoch: 55/100, Train Loss: 1.5563, Train Acc: 0.6049, Val Loss: 1.5570, Val Acc: 0.6032\n",
      "Epoch: 56/100, Train Loss: 1.5544, Train Acc: 0.6063, Val Loss: 1.5561, Val Acc: 0.6011\n",
      "Epoch: 57/100, Train Loss: 1.5551, Train Acc: 0.6054, Val Loss: 1.5601, Val Acc: 0.5982\n",
      "Epoch: 58/100, Train Loss: 1.5555, Train Acc: 0.6062, Val Loss: 1.5603, Val Acc: 0.5977\n",
      "Epoch: 59/100, Train Loss: 1.5526, Train Acc: 0.6085, Val Loss: 1.5587, Val Acc: 0.5971\n",
      "Epoch: 60/100, Train Loss: 1.5548, Train Acc: 0.6059, Val Loss: 1.5574, Val Acc: 0.6001\n",
      "Epoch: 61/100, Train Loss: 1.5557, Train Acc: 0.6041, Val Loss: 1.5592, Val Acc: 0.5997\n",
      "Epoch: 62/100, Train Loss: 1.5563, Train Acc: 0.6024, Val Loss: 1.5559, Val Acc: 0.6048\n",
      "Epoch: 63/100, Train Loss: 1.5561, Train Acc: 0.6049, Val Loss: 1.5598, Val Acc: 0.5984\n",
      "Epoch: 64/100, Train Loss: 1.5558, Train Acc: 0.6043, Val Loss: 1.5606, Val Acc: 0.5993\n",
      "Epoch: 65/100, Train Loss: 1.5532, Train Acc: 0.6082, Val Loss: 1.5550, Val Acc: 0.6056\n",
      "Epoch: 66/100, Train Loss: 1.5564, Train Acc: 0.6052, Val Loss: 1.5615, Val Acc: 0.5963\n",
      "Epoch: 67/100, Train Loss: 1.5573, Train Acc: 0.6032, Val Loss: 1.5580, Val Acc: 0.5996\n",
      "Epoch: 68/100, Train Loss: 1.5556, Train Acc: 0.6045, Val Loss: 1.5591, Val Acc: 0.5985\n",
      "Epoch: 69/100, Train Loss: 1.5551, Train Acc: 0.6055, Val Loss: 1.5557, Val Acc: 0.6036\n",
      "Epoch: 70/100, Train Loss: 1.5569, Train Acc: 0.6038, Val Loss: 1.5554, Val Acc: 0.6025\n",
      "Epoch: 71/100, Train Loss: 1.5588, Train Acc: 0.6011, Val Loss: 1.5613, Val Acc: 0.5978\n",
      "Epoch: 72/100, Train Loss: 1.5555, Train Acc: 0.6050, Val Loss: 1.5595, Val Acc: 0.6015\n",
      "Epoch: 73/100, Train Loss: 1.5555, Train Acc: 0.6061, Val Loss: 1.5567, Val Acc: 0.6012\n",
      "Epoch: 74/100, Train Loss: 1.5541, Train Acc: 0.6065, Val Loss: 1.5569, Val Acc: 0.6040\n",
      "Epoch: 75/100, Train Loss: 1.5569, Train Acc: 0.6028, Val Loss: 1.5579, Val Acc: 0.5997\n",
      "Epoch: 76/100, Train Loss: 1.5567, Train Acc: 0.6037, Val Loss: 1.5572, Val Acc: 0.6033\n",
      "Epoch: 77/100, Train Loss: 1.5559, Train Acc: 0.6054, Val Loss: 1.5558, Val Acc: 0.6023\n",
      "Epoch: 78/100, Train Loss: 1.5558, Train Acc: 0.6049, Val Loss: 1.5613, Val Acc: 0.5962\n",
      "Epoch: 79/100, Train Loss: 1.5543, Train Acc: 0.6065, Val Loss: 1.5589, Val Acc: 0.5989\n",
      "Epoch: 80/100, Train Loss: 1.5566, Train Acc: 0.6038, Val Loss: 1.5584, Val Acc: 0.6003\n",
      "Epoch: 81/100, Train Loss: 1.5554, Train Acc: 0.6042, Val Loss: 1.5541, Val Acc: 0.6041\n",
      "Epoch: 82/100, Train Loss: 1.5577, Train Acc: 0.6015, Val Loss: 1.5580, Val Acc: 0.5996\n",
      "Epoch: 83/100, Train Loss: 1.5552, Train Acc: 0.6060, Val Loss: 1.5564, Val Acc: 0.6012\n",
      "Epoch: 84/100, Train Loss: 1.5567, Train Acc: 0.6044, Val Loss: 1.5556, Val Acc: 0.6029\n",
      "Epoch: 85/100, Train Loss: 1.5561, Train Acc: 0.6035, Val Loss: 1.5571, Val Acc: 0.6016\n",
      "Epoch: 86/100, Train Loss: 1.5568, Train Acc: 0.6034, Val Loss: 1.5591, Val Acc: 0.5996\n",
      "Epoch: 87/100, Train Loss: 1.5534, Train Acc: 0.6092, Val Loss: 1.5583, Val Acc: 0.5988\n",
      "Epoch: 88/100, Train Loss: 1.5549, Train Acc: 0.6053, Val Loss: 1.5542, Val Acc: 0.6074\n",
      "Epoch: 89/100, Train Loss: 1.5545, Train Acc: 0.6070, Val Loss: 1.5604, Val Acc: 0.5973\n",
      "Epoch: 90/100, Train Loss: 1.5576, Train Acc: 0.6014, Val Loss: 1.5540, Val Acc: 0.6060\n",
      "Epoch: 91/100, Train Loss: 1.5543, Train Acc: 0.6066, Val Loss: 1.5574, Val Acc: 0.6003\n",
      "Epoch: 92/100, Train Loss: 1.5563, Train Acc: 0.6036, Val Loss: 1.5597, Val Acc: 0.6000\n",
      "Epoch: 93/100, Train Loss: 1.5542, Train Acc: 0.6060, Val Loss: 1.5609, Val Acc: 0.5971\n",
      "Epoch: 94/100, Train Loss: 1.5568, Train Acc: 0.6033, Val Loss: 1.5577, Val Acc: 0.6001\n",
      "Epoch: 95/100, Train Loss: 1.5562, Train Acc: 0.6042, Val Loss: 1.5565, Val Acc: 0.6011\n",
      "Epoch: 96/100, Train Loss: 1.5548, Train Acc: 0.6067, Val Loss: 1.5578, Val Acc: 0.6003\n",
      "Epoch: 97/100, Train Loss: 1.5554, Train Acc: 0.6052, Val Loss: 1.5568, Val Acc: 0.6001\n",
      "Epoch: 98/100, Train Loss: 1.5554, Train Acc: 0.6053, Val Loss: 1.5588, Val Acc: 0.6007\n",
      "Epoch: 99/100, Train Loss: 1.5552, Train Acc: 0.6060, Val Loss: 1.5560, Val Acc: 0.6036\n",
      "Epoch: 100/100, Train Loss: 1.5554, Train Acc: 0.6056, Val Loss: 1.5582, Val Acc: 0.5997\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create empty lists to store loss and accuracy for each epoch\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_dataloader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # append the loss and accuracy to the lists\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d739b73-3926-4de7-94eb-ec7cd6d2bdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model saved as model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-050e0aaa9f1e>:17: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scaled_attention_logits = qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
      "<ipython-input-11-050e0aaa9f1e>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scaled_attention_logits = qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# Assuming your model is defined and named 'model'\n",
    "# Replace 'model' with the actual name of your model instance\n",
    "\n",
    "# Check the device of the model\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# Create a dummy input tensor of the same shape as your actual input\n",
    "# Use a single input for the dummy input tensor, so the shape will be [1, 3, 48, 48]\n",
    "dummy_input = torch.randn(1, 3, 48, 48).to(device)\n",
    "\n",
    "# Export the model to an ONNX file\n",
    "onnx_filename = \"model.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_filename)\n",
    "\n",
    "print(f\"Model saved as {onnx_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3ba05-e6dc-4393-877b-a795bc38c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FacialExpressionDetectionModel(num_classes, num_heads).to(device)\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.RMSprop(model2.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler2 = optim.lr_scheduler.StepLR(optimizer2, step_size=10, gamma=0.1)\n",
    "scaler2 = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432376b6-49df-4e14-9d0d-24796b5ddbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create empty lists to store loss and accuracy for each epoch\n",
    "train_loss_list2 = []\n",
    "train_acc_list2 = []\n",
    "val_loss_list2 = []\n",
    "val_acc_list2 = []\n",
    "\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model2, train_dataloader, criterion, optimizer2, device)\n",
    "    val_loss, val_acc = validate_epoch(model2, val_dataloader, criterion, device)\n",
    "    scheduler2.step(val_loss)\n",
    "    \n",
    "    # append the loss and accuracy to the lists\n",
    "    train_loss_list2.append(train_loss)\n",
    "    train_acc_list2.append(train_acc)\n",
    "    val_loss_list2.append(val_loss)\n",
    "    val_acc_list2.append(val_acc)\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c588ad5-92e0-4a33-9286-ea60e249e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = FacialExpressionDetectionModel(num_classes, num_heads).to(device)\n",
    "criterion3 = nn.CrossEntropyLoss()\n",
    "scaler3 = GradScaler()\n",
    "optimizer3 = optim.SGD(model3.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler3 = optim.lr_scheduler.CosineAnnealingLR(optimizer3, T_max=50, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483c960-2fab-47b2-91e4-39f10e19561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create empty lists to store loss and accuracy for each epoch\n",
    "train_loss_list3 = []\n",
    "train_acc_list3 = []\n",
    "val_loss_list3 = []\n",
    "val_acc_list3 = []\n",
    "\n",
    "num_epochs = 80\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model3, train_dataloader, criterion3, optimizer3, device)\n",
    "    val_loss, val_acc = validate_epoch(model3, val_dataloader, criterion3, device)\n",
    "    scheduler3.step(val_loss)\n",
    "    \n",
    "    # append the loss and accuracy to the lists\n",
    "    train_loss_list3.append(train_loss)\n",
    "    train_acc_list3.append(train_acc)\n",
    "    val_loss_list3.append(val_loss)\n",
    "    val_acc_list3.append(val_acc)\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6184e-fabc-4f4a-afd0-309d55b3f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc_np = [t.cpu().numpy() for t in train_acc_list]\n",
    "val_acc_np = [t.cpu().numpy() for t in val_acc_list]\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=train_acc_np, label='Train')\n",
    "sns.lineplot(data=val_acc_np, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# convert lists to dataframes\n",
    "train_acc_df = pd.DataFrame({'accuracy': train_acc_list})\n",
    "val_acc_df = pd.DataFrame({'accuracy': val_acc_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db09c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to check pytorch version\n",
    "def check_pytorch_version():\n",
    "    if torch.__version__ >= '1.6.0':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check cuda version\n",
    "def check_cuda_version():\n",
    "    if torch.cuda.is_available():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check cudnn version\n",
    "def check_cudnn_version():\n",
    "    if check_cuda_version():\n",
    "        if torch.backends.cudnn.enabled:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define a function to check if the system is ready for training\n",
    "def check_system():\n",
    "    if check_pytorch_version():\n",
    "        print('PyTorch version: {}'.format(torch.__version__))\n",
    "    else:\n",
    "        print('PyTorch version: {} (update required)'.format(torch.__version__))\n",
    "        \n",
    "    if check_cuda_version():\n",
    "        print('CUDA version: {}'.format(torch.version.cuda))\n",
    "    else:\n",
    "        print('CUDA version: {} (install CUDA to enable GPU training)'.format(torch.version.cuda))\n",
    "        \n",
    "    if check_cudnn_version():\n",
    "        print('cuDNN version: {}'.format(torch.backends.cudnn.version()))\n",
    "    else:\n",
    "        print('cuDNN version: {} (install cuDNN to enable GPU training)'.format(torch.backends.cudnn.version()))\n",
    "\n",
    "check_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bf957-012e-41bb-838a-a2e7b9efeae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c838d3-272d-4749-a4bd-d2fbb9035436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cfe33-7a05-45cd-b35e-f21a874db635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
